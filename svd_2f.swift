import TensorFlow
import Foundation

// Custom differentiable type.
struct Decomposer: Differentiable {
    var p: Tensor<Float>
    var q: Tensor<Float>
    func callAsFunction() -> Tensor<Float> {
        return matmul(p, q)
    }
}

//let A: Tensor<Float> = [[0,1,2],[3,_TensorElementLiteral(floatLiteral:Float.nan),5],[6,7,8]]
//let A: Tensor<Float> = [[0,1,2],[3,nan,5],[6,7,8]]
let A: Tensor<Float> = [[0,1,2],[3,40,5],[6,7,8]]

let p: Tensor<Float> = [[1,1],[1,1],[1,1]]
let q: Tensor<Float> = [[1,1,1],[1,1,1]]

var model = Decomposer(p: p, q: q)
print(model())
print(A)

let optimizer = SGD(for: model, learningRate: 0.01)

// SGD Training loop
for _ in 0..<1000 {
    // p updating step
    var ùõÅmodel = model.gradient { model -> Tensor<Float> in
	let √Ç = model()
	let loss = (√Ç-A).squared().mean()
        return loss
    }
    ùõÅmodel.q *= [[0,0,0],[1,1,1]]
    optimizer.update(&model, along: ùõÅmodel)
    
    ùõÅmodel = model.gradient { model -> Tensor<Float> in
	let √Ç = model()
	let loss = (√Ç-A).squared().mean()
        return loss
    }
    ùõÅmodel.q *= [[1,1,1],[0,0,0]]
    optimizer.update(&model, along: ùõÅmodel)
    
    // q updating step
    ùõÅmodel = model.gradient { model -> Tensor<Float> in
	let √Ç = model()
	let loss = (√Ç-A).squared().mean()
        print("Loss: \(loss)")
        return loss
    }
    ùõÅmodel.p *= [[0,1],[0,1],[0,1]]
    optimizer.update(&model, along: ùõÅmodel)
    	
    ùõÅmodel = model.gradient { model -> Tensor<Float> in
	let √Ç = model()
	let loss = (√Ç-A).squared().mean()
        print("Loss: \(loss)")
        return loss
    }
    ùõÅmodel.p *= [[1,0],[1,0],[1,0]]
    optimizer.update(&model, along: ùõÅmodel)
}

print("Learned p tensor:, \(model.p)")
print("Learned q tensor:, \(model.q)")
print(model())
